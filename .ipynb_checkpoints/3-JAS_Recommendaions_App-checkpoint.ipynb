{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-b413e44b5826>\", line 204, in <module>\n",
      "    app.run(debug = True)\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/site-packages/flask/app.py\", line 841, in run\n",
      "    run_simple(host, port, self, **options)\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/site-packages/werkzeug/serving.py\", line 795, in run_simple\n",
      "    s.bind(get_sockaddr(hostname, port, address_family))\n",
      "OSError: [Errno 98] Address already in use\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/posixpath.py\", line 386, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/douglas/anaconda3/lib/python3.6/posixpath.py\", line 401, in _joinrealpath\n",
      "    if isabs(rest):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 98] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#working copy \n",
    "\n",
    "import flask #from flask import Flask, render_template, request, flash\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from forms import EntryForm\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import io\n",
    "import base64\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "#---------- MODEL IN MEMORY ----------------#\n",
    "#  more stop words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def noun_tokenizer(text):\n",
    "\n",
    "    # remove punctuation\n",
    "    punct = string.punctuation + '±−≤°≥“”'\n",
    "    remove_punct = str.maketrans('', '', punct)\n",
    "    text = text.translate(remove_punct)\n",
    "\n",
    "    # remove digits and convert to lower case\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    text = text.lower().translate(remove_digits)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    is_noun = lambda pos: (pos == 'NN') | (pos == 'NNS') #only nouns - no proper nouns\n",
    "    nouns = [word for (word, pos) in pos_tag(tokens) if is_noun(pos)]\n",
    "    # remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    JAS_words = ['use', 'model', 'anim', 'method', 'result', 'kg', 'vs', 'treatment', 'mgkg', 'per', 'h', 'x']\n",
    "    more_words = ['et', 'al', 'mg', 'cm', 'animals', 'animal', 'mm', 'experiment','treatments','containing', 'studies', 'added', 'sources', 'total', 'science', 'research', 'field', 'degree','report', 'greater', 'increased', 'decreased', 'less', 'use', 'production', 'agriculture']\n",
    "    stop_words = stop_words + JAS_words + more_words\n",
    "    tokens_stop = [y for y in tokens if y not in stop_words]\n",
    "\n",
    "    # stem\n",
    "    #stemmer = SnowballStemmer('english')\n",
    "    #tokens_stem = [stemmer.stem(y) for y in tokens_stop] \n",
    "\n",
    "    return tokens_stop\n",
    "\n",
    "# Load pickled corpus:\n",
    "with open('jas_df.pkl', 'rb') as picklefile:\n",
    "    jas_df = pickle.load(picklefile)\n",
    "with open('jas_abstracts.pkl', 'rb') as picklefile:\n",
    "    jas_abstracts = pickle.load(picklefile)\n",
    "\n",
    "#Load NLP \n",
    "with open('JAS_vocab.pkl', 'rb') as picklefile:\n",
    "    JAS_vocab = pickle.load(picklefile)\n",
    "with open('JAS_nmf.pkl', 'rb') as picklefile:\n",
    "    JAS_nmf = pickle.load(picklefile)\n",
    "with open('JAS_tfidf.pkl', 'rb') as picklefile:\n",
    "    JAS_tfidf = pickle.load(picklefile)\n",
    "with open('JAS_vecs.pkl', 'rb') as picklefile:\n",
    "    JAS_vecs = pickle.load(picklefile)\n",
    "\n",
    "nmf_topic_words100 = [topic[:100] for topic in np.argsort(-JAS_nmf.components_)] #list of topics' indices for top 100 words\n",
    "#create a dictionary of top 100 words?  and load that instead of JAS_vocab\n",
    "    \n",
    "#---------- FUNCTIONS IN MEMORY ----------------#    \n",
    "\n",
    "\n",
    "    \n",
    "def top_topics_for_article(art_num):\n",
    "    a = np.where(JAS_vecs[art_num] > 0.01)[0] #list of indices where nmf coeff is over 0.01 for a given article\n",
    "    b = JAS_vecs[art_num][JAS_vecs[art_num] > 0.01] # list of corresponding nmf coeffs for a given article\n",
    "    return a,b\n",
    "\n",
    "def topic_word_clouds(art_idx):\n",
    "    '''input:\n",
    "    art_idx = an article index\n",
    "    rows = number of topics with nmf coeff > 0.01'''\n",
    "    fig = plt.figure() # create figure to add subplots to\n",
    "    #columns = 1\n",
    "    #rows = len(art_indices)\n",
    "    #for art_idx in art_indices:\n",
    "    topic_nums, topic_coeffs = top_topics_for_article(art_idx) \n",
    "    rows = len(topic_nums)\n",
    "    for i, topic_idx in enumerate(topic_nums):\n",
    "        topic_words = \" \".join([JAS_vocab[word_idx].strip() for word_idx in nmf_topic_words100[topic_idx]])\n",
    "        wordcloud = WordCloud(collocations=False).generate(topic_words)\n",
    "        fig.add_subplot(rows, 1, i+1)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    img = io.BytesIO() #buffer\n",
    "    plt.savefig(img, format='png') #save to buffer\n",
    "    img.seek(0) #start at right spot\n",
    "    plot_url = base64.b64encode(img.getvalue()).decode() #copied\n",
    "    \n",
    "    #topic_\n",
    "    return plot_url\n",
    "\n",
    "\n",
    "def filter_top5_recommender(article, vectorizer, NMF, vecs, metric, keyword=None, num_results=6):\n",
    "    #tokenize given article using trained vectorizer\n",
    "    tokens = vectorizer.transform(article)\n",
    "    vec = NMF.transform(tokens)[0] #nmf.transform returns a nested list\n",
    "    \n",
    "    #select distance metric \n",
    "    if metric == 'C':\n",
    "        dists = [cosine(vec, vecs[i]) for i in range(len(vecs))]\n",
    "    else:\n",
    "        dists = [euclidean(vec, vecs[i]) for i in range(len(vecs))]\n",
    "    \n",
    "    #Number of results to return\n",
    "    if num_results:      \n",
    "        num_results = int(num_results)\n",
    "    else: \n",
    "        num_results = 6  #set default if not given\n",
    "        \n",
    "    #Filter or not?\n",
    "    articles = []\n",
    "    if keyword:        \n",
    "        i = 0\n",
    "        while len(articles) < num_results:\n",
    "            idx = np.argsort(cos_dists)[i] #search closest abstracts for the key word\n",
    "            if re.search(re.compile('(?i){}'.format(keyword)), jas_abstracts[idx]): #if yes, add article\n",
    "                cloud_plot = topic_word_clouds(idx)\n",
    "                article = {'rank': i, 'distance': '%.3f' % dists[idx], 'title': jas_df['title'].iloc[idx],\n",
    "                            'abstract_txt': jas_abstracts.iloc[idx], 'doi_link': jas_df['doi'].iloc[idx],\n",
    "                            'cloud': cloud_plot}\n",
    "                articles.append(article) \n",
    "            i += 1 #else, move on\n",
    "    else:\n",
    "        close_idx = np.argsort(cos_dists)[:num_results] #just grab the closest abstracts\n",
    "        for i, idx in enumerate(close_idx):\n",
    "            cloud_plot = topic_word_clouds(idx)\n",
    "            article = {'rank': i, 'distance': '%.3f' % dists[idx], 'title': jas_df['title'].iloc[idx],\n",
    "                        'abstract_txt': jas_abstracts.iloc[idx], 'doi_link': jas_df['doi'].iloc[idx],\n",
    "                        'cloud': cloud_plot}\n",
    "            articles.append(article)\n",
    "    return articles # a list of dictionaries\n",
    "    \n",
    "\n",
    "#---------- URLS AND WEB PAGES -------------#\n",
    "\n",
    "# Initialize the app\n",
    "app = flask.Flask(__name__)\n",
    "app.secret_key = 'development key'\n",
    "\n",
    "#Add enumerate function for jinga\n",
    "@app.context_processor\n",
    "def inject_enumerate():\n",
    "    return dict(enumerate=enumerate)\n",
    "\n",
    "# Homepage\n",
    "@app.route(\"/\")\n",
    "def search_page():\n",
    "    \"\"\"\n",
    "    Search page: serve our form page, contact.html\n",
    "    \"\"\"\n",
    "    form = EntryForm()\n",
    "    return flask.render_template('contact.html', form = form)\n",
    "\n",
    "# Get an example and return it's score from the predictor model\n",
    "@app.route(\"/contact\", methods=[\"POST\"])\n",
    "def contact():\n",
    "    \"\"\"\n",
    "    When A POST request with json data is made to this uri,\n",
    "    Read the example from the json, predict probability and\n",
    "    send it with a response\n",
    "    \"\"\"\n",
    "    # Get data from form that came with the request\n",
    "    data = flask.request.form\n",
    "    #has attributes og_abstract similarity keywords num_results\n",
    "    result = filter_top5_recommender([data['og_abstract']], JAS_tfidf, JAS_nmf, JAS_vecs, \n",
    "                                       metric=data['similarity'], keyword=data['keyword'], \n",
    "                                       num_results=data['num_results'])\n",
    "    \n",
    "    # Put the result in a nice dict so we can send it as json\n",
    "    \n",
    "    #return flask.render_template(\"results.html\",result = result)\n",
    "\n",
    "   \n",
    "    return flask.render_template(\"results.html\",result = result) #, plot_url = plot_url)\n",
    "\t#return '<img src=\"data:image/png;base64,{}\">'.format(plot_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------- RUN WEB APP SERVER ------------#\n",
    "\n",
    "# Start the app server on port 80\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   app.run(debug = True)\n",
    "# (The default website port)\n",
    "app.run(host='0.0.0.0')\n",
    "app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reserve copy\n",
    "\n",
    "import flask #from flask import Flask, render_template, request, flash\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from forms import EntryForm\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import io\n",
    "import base64\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "#---------- MODEL IN MEMORY ----------------#\n",
    "#  more stop words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def noun_tokenizer(text):\n",
    "\n",
    "    # remove punctuation\n",
    "    punct = string.punctuation + '±−≤°≥“”'\n",
    "    remove_punct = str.maketrans('', '', punct)\n",
    "    text = text.translate(remove_punct)\n",
    "\n",
    "    # remove digits and convert to lower case\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    text = text.lower().translate(remove_digits)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    is_noun = lambda pos: (pos == 'NN') | (pos == 'NNS') #only nouns - no proper nouns\n",
    "    nouns = [word for (word, pos) in pos_tag(tokens) if is_noun(pos)]\n",
    "    # remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    JAS_words = ['use', 'model', 'anim', 'method', 'result', 'kg', 'vs', 'treatment', 'mgkg', 'per', 'h', 'x']\n",
    "    more_words = ['et', 'al', 'mg', 'cm', 'animals', 'animal', 'mm', 'experiment','treatments','containing', 'studies', 'added', 'sources', 'total', 'science', 'research', 'field', 'degree','report', 'greater', 'increased', 'decreased', 'less', 'use', 'production', 'agriculture']\n",
    "    stop_words = stop_words + JAS_words + more_words\n",
    "    tokens_stop = [y for y in tokens if y not in stop_words]\n",
    "\n",
    "    # stem\n",
    "    #stemmer = SnowballStemmer('english')\n",
    "    #tokens_stem = [stemmer.stem(y) for y in tokens_stop] \n",
    "\n",
    "    return tokens_stop\n",
    "\n",
    "# Load pickled corpus:\n",
    "with open('jas_df.pkl', 'rb') as picklefile:\n",
    "    jas_df = pickle.load(picklefile)\n",
    "with open('jas_abstracts.pkl', 'rb') as picklefile:\n",
    "    jas_abstracts = pickle.load(picklefile)\n",
    "\n",
    "#Load NLP \n",
    "with open('JAS_vocab.pkl', 'rb') as picklefile:\n",
    "    JAS_vocab = pickle.load(picklefile)\n",
    "with open('JAS_nmf.pkl', 'rb') as picklefile:\n",
    "    JAS_nmf = pickle.load(picklefile)\n",
    "with open('JAS_tfidf.pkl', 'rb') as picklefile:\n",
    "    JAS_tfidf = pickle.load(picklefile)\n",
    "with open('JAS_vecs.pkl', 'rb') as picklefile:\n",
    "    JAS_vecs = pickle.load(picklefile)\n",
    "\n",
    "nmf_topic_words100 = [topic[:100] for topic in np.argsort(-JAS_nmf.components_)]\n",
    "    \n",
    "#---------- FUNCTIONS IN MEMORY ----------------#    \n",
    "\n",
    "\n",
    "    \n",
    "def top_topics_for_article(art_num):\n",
    "    a = np.where(JAS_vecs[art_num] > 0.01)[0]\n",
    "    b = JAS_vecs[art_num][JAS_vecs[art_num] > 0.01]\n",
    "    return a,b\n",
    "\n",
    "def topic_word_clouds(art_idx, rows):\n",
    "    '''input should be a list of article indices'''\n",
    "    fig = plt.figure()\n",
    "    #columns = 1\n",
    "    #rows = len(art_indices)\n",
    "    #for art_idx in art_indices:\n",
    "    topic_nums, topic_coeffs = top_topics_for_article(art_idx)\n",
    "    for i, topic_idx in enumerate(topic_nums):\n",
    "        topic_words = \" \".join([JAS_vocab[word_idx].strip() for word_idx in nmf_topic_words100[topic_idx]])\n",
    "        wordcloud = WordCloud(collocations=False).generate(topic_words)\n",
    "        fig.add_subplot(rows, 1, i+1)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    img = io.BytesIO()\n",
    "    plt.savefig(img, format='png')\n",
    "    img.seek(0)\n",
    "    plot_url = base64.b64encode(img.getvalue()).decode()\n",
    "    return plot_url\n",
    "\n",
    "\n",
    "def filter_top5_recommender(article, vectorizer, NMF, vecs, keyword=None, num_results=6):\n",
    "    tokens = vectorizer.transform(article)\n",
    "    vec = NMF.transform(tokens)[0] #nmf.transform returns a nested list\n",
    "\n",
    "    cos_dists = [cosine(vec, vecs[i]) for i in range(len(vecs))]\n",
    "    #print(np.argsort(cos_dists))\n",
    "\n",
    "    num_results = int(num_results)\t\n",
    "    if keyword:\n",
    "        articles = []\n",
    "        i = 0\n",
    "        while len(articles) < num_results:\n",
    "            idx = np.argsort(cos_dists)[i]\n",
    "            if re.search(re.compile('(?i){}'.format(keyword)), jas_abstracts[idx]):\n",
    "                articles.append((i, idx, cos_dists[idx], jas_abstracts[idx]))\n",
    "            i += 1\n",
    "        return articles[1:num_results]\n",
    "    else:\n",
    "        five_idx = np.argsort(cos_dists)[1:num_results]\n",
    "        return [(i, idx, cos_dists[idx], jas_abstracts.iloc[idx]) for i, idx in enumerate(five_idx)]\n",
    "\n",
    "def filter_top5_recommender2(article, vectorizer, NMF, vecs, keyword=None, num_results=6):\n",
    "    tokens = vectorizer.transform(article)\n",
    "    vec = NMF.transform(tokens)\n",
    "    \n",
    "    euc_dists = [euclidean(vec, vecs[i]) for i in range(len(vecs))]\n",
    "\n",
    "    num_results = int(num_results)\n",
    "    if keyword:\n",
    "        articles = []\n",
    "        i = 0\n",
    "        while len(articles) < num_results:\n",
    "            idx = np.argsort(euc_dists)[i]\n",
    "            if re.search(re.compile('(?i){}'.format(keyword)), jas_abstracts[idx]):\n",
    "                articles.append((i, idx, euc_dists[idx], jas_abstracts[idx]))\n",
    "            i += 1\n",
    "        return articles[1:num_results]\n",
    "    else:\n",
    "        five_idx = np.argsort(euc_dists)[:num_results]\n",
    "   \n",
    "        return [(i, idx, euc_dists[idx], jas_abstracts.iloc[idx], topic_word_clouds(idx, len(five_idx))) for i, idx in enumerate(five_idx)]\n",
    "    \n",
    "#---------- URLS AND WEB PAGES -------------#\n",
    "\n",
    "# Initialize the app\n",
    "app = flask.Flask(__name__)\n",
    "app.secret_key = 'development key'\n",
    "\n",
    "#Add enumerate function\n",
    "@app.context_processor\n",
    "def inject_enumerate():\n",
    "    return dict(enumerate=enumerate)\n",
    "\n",
    "# Homepage\n",
    "@app.route(\"/\")\n",
    "def search_page():\n",
    "    \"\"\"\n",
    "    Homepage: serve our visualization page, awesome.html\n",
    "    \"\"\"\n",
    "    form = EntryForm()\n",
    "    return flask.render_template('contact.html', form = form)\n",
    "\n",
    "# Get an example and return it's score from the predictor model\n",
    "@app.route(\"/contact\", methods=[\"POST\"])\n",
    "def contact():\n",
    "    \"\"\"\n",
    "    When A POST request with json data is made to this uri,\n",
    "    Read the example from the json, predict probability and\n",
    "    send it with a response\n",
    "    \"\"\"\n",
    "    # Get decision score for our example that came with the request\n",
    "    data = flask.request.form\n",
    "    #has attributes og_abstract similarity keywords num_results\n",
    "    if data['similarity'] == 'C':\n",
    "        result = filter_top5_recommender([data['og_abstract']], JAS_tfidf, JAS_nmf, JAS_vecs, keyword=data['keywords'], num_results=data['num_results'])\n",
    "    else:\n",
    "        result = filter_top5_recommender2([data['og_abstract']], JAS_tfidf, JAS_nmf, JAS_vecs, keyword=data['keywords'], num_results=data['num_results'])\n",
    "    # Put the result in a nice dict so we can send it as json\n",
    "    \n",
    "    #return flask.render_template(\"results.html\",result = result)\n",
    "\n",
    "   \n",
    "    return flask.render_template(\"results.html\",result = result) #, plot_url = plot_url)\n",
    "\t#return '<img src=\"data:image/png;base64,{}\">'.format(plot_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------- RUN WEB APP SERVER ------------#\n",
    "\n",
    "# Start the app server on port 80\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   app.run(debug = True)\n",
    "# (The default website port)\n",
    "app.run(host='0.0.0.0')\n",
    "app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
